#!/bin/bash
##shellcheck disable=SC1035,SC1072
#
!clearrows
!SCRIPT_START=$SECONDS
!TW_SPEED=xxxfast
!TW_INDENT=0
decp SCRIPT_START TW_SPEED TW_INDENT


cowsay -f apt 'hello, world!' | tw



(spacetime; whoami; pwd) |tw



!clearrows
!TW_INDENT=8
!figlet 'YaTTI' |tw

!tw "Yayasan Teknologi Terbuka Indonesia"

!tw "Indonesian Open Technology Foundation"

!tw "https://yatti.id/"

!
!tw "Knowledgebase Creation Using 'CustomKB'" -s xxfast
!

!
!toilet -f wideterm 'CustomKB' |typewriter -s xxfast
##!figlet -f digital 'CustomKB' |tw -s xxfast
!

!
!cowsay -f www "An Overview" |tw -s xxfast
!


# This is a description of the 'CustomKB' knowledgebase creation and query system.
#
# It is as much about protocols and process as it is code.
#
# Building a knowledgebase requires the following broad processes:
!cowsay -f www "Domain, Ingestion, Digestion, Segmentation, Embedding, Query" |tw
#
# Domain Parameters
#   - Focus: what is the area of focus for this knowledebase?
#   - Scope: is the scope of focus narrower, or wider?
#
# Ingestion of Raw Data
#   - Curation (selection of data sources)
#   - Acquiring (downloading, etc)
#   - Post-Processing (Manual/Auto)
#  - Ingestion: obtaining raw data in text formats
# Selection and acquiring of raw data (websites, books, articles, transcripts, etc) that can be converted to markdown text formats.
#
# Digestion of Raw Data
#   - Conversion to markdown
#   - Markdown cleanup
#   - Post-Processing (Manual/Auto)
# Conversion of raw content into markdown formatted files.
#
# Segmentation of Markup
# All files in the data staging directory are segmented into fields of 300-500 tokens each.
#
# Stopword fields for the relevant language are also generated at this point.
#
#
# Embedding Vector Database
# Once data is in the database, embeddings are generated for all of the segments, and these are stored in a faiss vector database.
#

# Query
# Once embedding has completed, the knowledgebase can be used to obtain context segments, that can be included as context in LLM queries.
#
#
!exit


!TW_INDENT=0

# But first, let's review what we mean by "knowledgebases" in the context of RAG systems
!
!cowthink -f apt 'what is a knowledgebase?' |tw
!

# In Retrieval-Augmented Generation (RAG) systems, a knowledgebase is a structured or semi-structured collection of information that serves as the source of domain-specific knowledge from which the system retrieves relevant information.

!cowthink -f apt "break this down for me"
!

# Key aspects of a RAG knowledgebase:
#
# 1. Content Structure (the Domain)
#    - Documents
#    - Articles
#    - Technical manuals
#    - FAQs
#    - Product specifications
#    - Internal documentation
#    - Structured data (databases)
#    - Unstructured text
#

# 2. Core Functions (Storage, Embeddings, Query)
# # Conceptual example of knowledgebase interaction
#    # Convert document to embeddings
#    create_embeddings(document)
#    # Store in vector database
#    VectorStore()
#    # Find relevant documents based on similarity
#    DocumentStore()
#

# 3. Key Characteristics:
#
# - Searchability: Optimized for quick retrieval of relevant information
# - Updatability: Can be modified and expanded as new information becomes available
# - Contextual Relevance: Contains domain-specific information
# - Vector Representation: Documents are typically converted to vector embeddings
# - Semantic Understanding: Supports meaning-based rather than just keyword matching
#

# 4. Common Implementation Components:
#
# - Vector Database (e.g., faiss, Pinecone)
# - Document Processors
# - Embedding Models
# - Retrieval Mechanisms
# - Index Structures
#

# 5. Usage in RAG Pipeline:
#
#     A[User Query] --> B[Query Processing]
#     B --> C[Knowledge Retrieval]
#     C --> D[Context Integration]
#     D --> E[Response Generation]
#     F[Knowledgebase] --> C
#

# The knowledgebase is fundamentally different from traditional databases in that it's optimized for:
#
# 1. Semantic Search: Understanding meaning, not just matching keywords
# 2. Contextual Retrieval: Finding relevant information based on context
# 3. Vector Similarity: Using embedding-based similarity searches
# 4. Flexible Content: Handling both structured and unstructured data
#

# The knowledgebase in RAG systems serves as the foundation for providing accurate, contextual, and relevant information to augment the language model's responses. It helps ground the model's outputs in factual, up-to-date information while reducing hallucinations and improving response accuracy.



!clearrows
!TW_SPEED=xxxfast
!cowsay -f www "customkb knowledgebase creation and query"
!
# Uses programs 'customkb' and 'dv2' (soon to be available open source on github).
#


customkb --help |tw


#
# This presentation is an overview of how we turn website
# content into a knowledgebase using these tools.
#

# The data in a knowledgebase is used as the basis for
# creating a vector database, and this in turn is used for
# query-similarity matching.  Matching data is sent as
# references in LLM queries.
#


# Creation of a knowledgebase involves these main processes:
#

!cowsay -f www "Ingestion, Digestion, Segmentation, Embedding, Query" |tw

#
#  - Ingestion: obtaining raw data in text formats (websites, books, articles, transcripts, etc)
#

#  - Digestion: selection and conversion of content into markdown formatted files, and moving them into the data staging directory.
#

#  - Segmentation: All files in the data staging directory are segmented into fields of around 400 tokens each.
#

#    Stopword fields for the relevant language are also generated at this point.
#

#  - Embedding: Once data is in the database, embeddings are generated for all of the segments, and these are stored in a faiss vector database.
#

#  - Query: Once embedding has completed, the knowledgebase can be used for queries.
#



#
# Let's start at the beginning, and show you around the simple file and directory structures.

pwd


declare -p VECTORDBS


cd $VECTORDBS


ll

# These are all the current knowledgebases available via api for research purposes



# All knowledgebases have their own config file, a file-segments database, and a vector database.



find -type f -name '*.cfg' |tw


find -type f -name '*.db' |tw


find -type f -name '*.faiss' |tw



dux -s |tw

#
# The relative filesizes of each of the knowledgebases can be seen above
#



# As an example, we shall examine the motivation.se knowledgebase


cd motivation.se/

ls -l |tw

ls -l |grep -v '.sh' |tw

ls -l |grep '.sh' |tw

ls -CF *.sh |tw

# In this example, the knowledgebase has been built from the website www.motivation.se, which is written completely in Swedish.



!cowsay -f apt "hej" |tw -s xxxfast
!


# This is the structure of the project directory 'motivation.se'.
tree -dL 3 |tw





ls -lA

# These are the files and directories within the knowledgebase root directory.
#





# All text and html files within the site have been converted to markdown format (.md) into the data-staging directory (embed_data.text/).


# In this use case, the knowledgebase is bilingual, with primary language Swedish.
#

# English translations of all Swedish text files are created using LLM gpt-4o-2024-11-20.
#

# Both languages are incorporated into into the resulting knowledgebase.
#



ls -lA |tw





!
!
!cowsay -f www "STEP 0: assemble the the data..." |tw -s xxxfast
!



#
# All .md and .txt files within directory 'embed_data/' have
# been imported into the knowledgebase data files.


cd embed_data


ls -lA |tw




cd html/


ls -lA |tw

# '0_dl_site.sh' is a wget wrapper script used to download an entire site, excepting image files.
#




tail -n 43 |tw -s xxxfast
#
# This is the important part of the wget download script.


#
# Once the site has been downloaded (into directory 'www.motivation.se') then the html files need to be  converted into markdown using the '1_create_md_files.sh' script.
#
4
cd ..
3
ls -l
3
ls en/ | head |tw -s xxxfast
5
ls sv/ | head |tw -s xxxfast
5
#
# The script '2_create_en_md_files.sh' is the swedish->english translation process. Output is stored in directory 'embed_data/en/'.

cd ..
2
ls -l |tw -s xxxfast
5
cd embed_data.text/
ls -l |tw -s xxxfast
5

# Within 'embed.text/*' are all the text data files that will be used to create the knowledgebase.

ls en/ | tail |tw -s xxxfast
5
ls sv/ | tail |tw -s xxxfast
5
# Once all the data is assembled, the knowledgebase database and vector files are built using the 'build' script.
#

!cowsay -f www "STEP 1: segment data; create vectors..." |tw -s xxxfast
3
#
# The text files have been segmented and imported into a structured database, and embeddings generated for the associated vector database.
#
2
# These are the final database and vector files.

cd ..

ls -l motivation.se.{cfg,db,faiss} |tw -s xxxfast
5
#
!cowsay -f www "STEP 1.5: TEST QUERIES" |tw -s xxxfast

#
# The 'test_queries.sh' script is used as a final proof before deployment. (Additional test prompts are still required.)
#

# The first query will be in Swedish, and the second in English.
#
3
./test_queries.sh
2
#
#

# Now let's try running a customkb queries directly from the command line. We'll redirect the output to into a file (/tmp/output.md).
#
3
customkb query motivation.se.cfg "Vilka är de huvudsakliga orsakerna till låg motivation bland män i städerna i moderna samhällen?" >/tmp/output.md
3
#
# Let's take a quick look at the Swedish output...
#
2

head -n 42 /tmp/output.md | md2ansi || true
3

customkb query motivation.se.cfg "What are the main causes of low motivation among urban men in modern societies?" >/tmp/output.md
3
#
# Let's look at the English output...
#
2

head -n 42 /tmp/output.md | md2ansi || true
3
#

#

# This concludes the overview of
# YaTTI's 'customkb' knowledgebase system
# at the terminal/backend level.
#

# 'customkb' is under active development.
# More integration and automation is planned.
# Respositories at https://yatti.id/
#

# We shall now look at this knowledgebase in operation on the wah.id website.
#

!cowthink -f apt "atlast frontend dostuff!" |tw -s xxxfast
!
3
# If you desire access to the semi-private knowledgebases on the wah.id LLM workbench (https://wah.id/), please send an email to garydean@yatti.id, subject "request login access".
# A passcode will be returned to you. This passcode is valid for 24 hours. You must register within this time. There is no cost.
#
# The only required registration information is your email address. Please allow time for assessment of your request. It is not automatic.
#
#
4
# launching web browser for https://wah.id/
google-chrome --new-window https://wah.id/

